"""
Adapter Generator - Auto-generate Scrapers from Patterns

Takes diagnosis results and detected patterns to automatically
generate Python collector modules that can be integrated into
the lead_intel_v2 pipeline.

No LLM required - uses templates and pattern matching.
"""

import json
import re
import os
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
from urllib.parse import urlparse

from src.utils.logger import get_logger

logger = get_logger(__name__)


class AdapterGenerator:
    """
    Generate Python collector adapters from detected patterns.
    Creates ready-to-use modules for the lead_intel_v2 pipeline.
    """
    
    # Template for API-based collector
    API_COLLECTOR_TEMPLATE = '''"""
Auto-generated collector for {domain}
Source: {source_url}
Generated: {timestamp}
Pattern: API-based extraction

This collector was automatically generated by the AutoDiscover Engine.
Edit as needed to customize extraction logic.
"""

import hashlib
import requests
from datetime import datetime
from typing import List, Dict, Optional

from src.utils.logger import get_logger
from src.utils.evidence import record_evidence

logger = get_logger(__name__)


class {class_name}:
    """
    Collector for {domain}
    
    Discovered API: {api_url}
    Data path: {data_path}
    Fields: {field_list}
    """
    
    API_URL = "{api_url}"
    DATA_PATH = "{data_path}"
    
    # Mapping from source fields to normalized lead fields
    FIELD_MAPPING = {field_mapping}
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({{
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "application/json",
        }})
        self.timeout = 30
        self.evidence_path = "outputs/evidence/evidence_log.csv"
    
    def harvest(self, target_iso3: Optional[List[str]] = None) -> List[Dict]:
        """
        Harvest leads from discovered API.
        
        Args:
            target_iso3: Optional country filter (ISO3 codes)
        
        Returns:
            List of normalized lead dictionaries
        """
        leads = []
        
        try:
            logger.info(f"{{self.__class__.__name__}}: fetching from {{self.API_URL}}")
            resp = self.session.get(self.API_URL, timeout=self.timeout)
            
            if resp.status_code != 200:
                logger.error(f"{{self.__class__.__name__}}: HTTP {{resp.status_code}}")
                return leads
            
            data = resp.json()
            
            # Extract items from detected path
            items = self._get_items(data)
            logger.info(f"{{self.__class__.__name__}}: found {{len(items)}} items")
            
            for item in items:
                lead = self._extract_lead(item)
                
                if not lead.get("company"):
                    continue
                
                # Country filter
                if target_iso3:
                    country = lead.get("country", "").lower()
                    # Simple filter - can be enhanced
                    if not any(iso.lower() in country for iso in target_iso3):
                        continue
                
                # Record evidence
                self._record_evidence(lead, item)
                
                leads.append(lead)
            
            logger.info(f"{{self.__class__.__name__}}: harvested {{len(leads)}} leads")
            
        except Exception as e:
            logger.error(f"{{self.__class__.__name__}}: error - {{e}}")
        
        return leads
    
    def _get_items(self, data: Any) -> List:
        """Extract list of items from API response."""
        if self.DATA_PATH == "root":
            return data if isinstance(data, list) else []
        
        # Navigate to nested path
        parts = self.DATA_PATH.split(".")
        current = data
        for part in parts:
            if isinstance(current, dict) and part in current:
                current = current[part]
            else:
                return []
        
        return current if isinstance(current, list) else []
    
    def _extract_lead(self, item: Dict) -> Dict:
        """Extract normalized lead from item."""
        lead = {{
            "source": self.API_URL,
            "source_type": "{source_type}",
            "source_name": "{source_name}",
        }}
        
        for source_field, target_field in self.FIELD_MAPPING.items():
            value = item.get(source_field)
            if value is not None:
                lead[target_field] = str(value).strip()
        
        return lead
    
    def _record_evidence(self, lead: Dict, raw_item: Dict):
        """Record evidence for audit trail."""
        snippet = f"{{lead.get('company', '')}} | {{lead.get('country', '')}}"
        content_hash = hashlib.md5(snippet.encode()).hexdigest()[:16]
        
        record_evidence(
            self.evidence_path,
            {{
                "source_type": "{source_type}",
                "source_name": "{source_name}",
                "url": self.API_URL,
                "title": lead.get("company", ""),
                "snippet": snippet[:400],
                "content_hash": content_hash,
                "fetched_at": datetime.utcnow().isoformat(timespec="seconds"),
            }},
        )


# CLI test
if __name__ == "__main__":
    collector = {class_name}()
    leads = collector.harvest()
    
    print(f"\\nHarvested {{len(leads)}} leads:")
    for lead in leads[:10]:
        print(f"  - {{lead.get('company', 'N/A')[:50]}} | {{lead.get('country', 'N/A')}}")
'''

    # Template for HTML-based collector
    HTML_COLLECTOR_TEMPLATE = '''"""
Auto-generated collector for {domain}
Source: {source_url}
Generated: {timestamp}
Pattern: HTML-based extraction

This collector was automatically generated by the AutoDiscover Engine.
Uses BeautifulSoup to extract data from HTML pages.
"""

import hashlib
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
import re

from src.utils.logger import get_logger
from src.utils.evidence import record_evidence

logger = get_logger(__name__)


class {class_name}:
    """
    HTML-based collector for {domain}
    
    URL Pattern: {url_pattern}
    List Selector: {list_selector}
    """
    
    BASE_URL = "{base_url}"
    LIST_URL = "{list_url}"
    
    # CSS Selectors detected
    LIST_SELECTOR = "{list_selector}"
    ITEM_SELECTOR = "{item_selector}"
    
    # Field selectors within each item
    FIELD_SELECTORS = {field_selectors}
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({{
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
        }})
        self.timeout = 30
        self.evidence_path = "outputs/evidence/evidence_log.csv"
    
    def harvest(self, target_iso3: Optional[List[str]] = None) -> List[Dict]:
        """Harvest leads from HTML pages."""
        leads = []
        
        try:
            logger.info(f"{{self.__class__.__name__}}: fetching {{self.LIST_URL}}")
            resp = self.session.get(self.LIST_URL, timeout=self.timeout)
            
            if resp.status_code != 200:
                logger.error(f"{{self.__class__.__name__}}: HTTP {{resp.status_code}}")
                return leads
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # Find items
            items = soup.select(self.ITEM_SELECTOR) if self.ITEM_SELECTOR else []
            logger.info(f"{{self.__class__.__name__}}: found {{len(items)}} items")
            
            for item in items:
                lead = self._extract_lead(item)
                
                if not lead.get("company"):
                    continue
                
                leads.append(lead)
            
            logger.info(f"{{self.__class__.__name__}}: harvested {{len(leads)}} leads")
            
        except Exception as e:
            logger.error(f"{{self.__class__.__name__}}: error - {{e}}")
        
        return leads
    
    def _extract_lead(self, item) -> Dict:
        """Extract lead from BeautifulSoup element."""
        lead = {{
            "source": self.LIST_URL,
            "source_type": "{source_type}",
            "source_name": "{source_name}",
        }}
        
        for field, selector in self.FIELD_SELECTORS.items():
            elem = item.select_one(selector) if selector else None
            if elem:
                lead[field] = elem.get_text(strip=True)
        
        # Try to extract email from text
        text = item.get_text()
        email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{{2,}}', text)
        if email_match and "email" not in lead:
            lead["email"] = email_match.group()
        
        return lead


# CLI test
if __name__ == "__main__":
    collector = {class_name}()
    leads = collector.harvest()
    
    print(f"\\nHarvested {{len(leads)}} leads:")
    for lead in leads[:10]:
        print(f"  - {{lead.get('company', 'N/A')[:50]}} | {{lead.get('country', 'N/A')}}")
'''

    def __init__(self, output_dir: str = "src/collectors/auto"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.adapters_index_path = Path("config/auto_adapters.yaml")
    
    def _make_class_name(self, domain: str) -> str:
        """Generate a Python class name from domain."""
        # Remove TLD and clean
        parts = domain.replace("www.", "").split(".")
        name = parts[0] if parts else "Unknown"
        # CamelCase
        name = re.sub(r"[^a-zA-Z0-9]+", " ", name)
        name = "".join(word.capitalize() for word in name.split())
        return f"{name}Collector"
    
    def _make_source_type(self, domain: str) -> str:
        """Generate source_type identifier."""
        name = domain.replace("www.", "").split(".")[0]
        return re.sub(r"[^a-z0-9]+", "_", name.lower())
    
    def generate_api_adapter(self, 
                             source_url: str,
                             api_url: str,
                             pattern: Dict) -> str:
        """
        Generate a collector module for an API-based source.
        
        Args:
            source_url: Original URL where API was discovered
            api_url: The API endpoint URL
            pattern: Pattern dict from PatternAnalyzer
        
        Returns:
            Path to generated module
        """
        domain = urlparse(source_url).netloc
        class_name = self._make_class_name(domain)
        source_type = self._make_source_type(domain)
        
        field_mapping = pattern.get("field_mapping", {})
        field_list = ", ".join(field_mapping.values())
        
        code = self.API_COLLECTOR_TEMPLATE.format(
            domain=domain,
            source_url=source_url,
            timestamp=datetime.utcnow().isoformat(),
            class_name=class_name,
            api_url=api_url,
            data_path=pattern.get("path", "root"),
            field_list=field_list or "company, country",
            field_mapping=json.dumps(field_mapping, indent=8),
            source_type=source_type,
            source_name=pattern.get("title", domain),
        )
        
        # Save module
        filename = f"{source_type}_collector.py"
        filepath = self.output_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(code)
        
        logger.info(f"Generated API adapter: {filepath}")
        
        # Update adapters index
        self._update_adapters_index(source_type, str(filepath), "api")
        
        return str(filepath)
    
    def generate_html_adapter(self,
                              source_url: str,
                              pattern: Dict) -> str:
        """
        Generate a collector module for an HTML-based source.
        
        Args:
            source_url: URL to scrape
            pattern: Pattern dict with selectors
        
        Returns:
            Path to generated module
        """
        parsed = urlparse(source_url)
        domain = parsed.netloc
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        class_name = self._make_class_name(domain)
        source_type = self._make_source_type(domain)
        
        code = self.HTML_COLLECTOR_TEMPLATE.format(
            domain=domain,
            source_url=source_url,
            timestamp=datetime.utcnow().isoformat(),
            class_name=class_name,
            base_url=base_url,
            list_url=source_url,
            url_pattern=source_url,
            list_selector=pattern.get("list_selector", ""),
            item_selector=pattern.get("item_selector", "div.item"),
            field_selectors=json.dumps(pattern.get("field_selectors", {}), indent=8),
            source_type=source_type,
            source_name=pattern.get("title", domain),
        )
        
        # Save module
        filename = f"{source_type}_collector.py"
        filepath = self.output_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(code)
        
        logger.info(f"Generated HTML adapter: {filepath}")
        
        # Update adapters index
        self._update_adapters_index(source_type, str(filepath), "html")
        
        return str(filepath)
    
    def _update_adapters_index(self, source_type: str, filepath: str, pattern_type: str):
        """Update the adapters index YAML."""
        import yaml
        
        index = {}
        if self.adapters_index_path.exists():
            with open(self.adapters_index_path, "r", encoding="utf-8") as f:
                index = yaml.safe_load(f) or {}
        
        if "auto_adapters" not in index:
            index["auto_adapters"] = {}
        
        index["auto_adapters"][source_type] = {
            "module": filepath,
            "type": pattern_type,
            "generated_at": datetime.utcnow().isoformat(),
            "enabled": False,  # Manual review required
        }
        
        with open(self.adapters_index_path, "w", encoding="utf-8") as f:
            yaml.dump(index, f, default_flow_style=False)
    
    def generate_from_diagnosis(self, diagnosis_dir: str) -> Optional[str]:
        """
        Generate adapter from a diagnosis session directory.
        
        Looks for valuable_apis.json and api_data.json from diagnoser.
        """
        diag_path = Path(diagnosis_dir)
        
        # Load diagnosis result
        diagnosis_file = diag_path / "diagnosis.json"
        if not diagnosis_file.exists():
            logger.error(f"No diagnosis.json in {diagnosis_dir}")
            return None
        
        with open(diagnosis_file, "r", encoding="utf-8") as f:
            diagnosis = json.load(f)
        
        source_url = diagnosis.get("url", "")
        
        # Check for API data
        api_data_file = diag_path / "api_data.json"
        if api_data_file.exists():
            with open(api_data_file, "r", encoding="utf-8") as f:
                api_data = json.load(f)
            
            if api_data:
                # Use the highest scored API
                best_api = api_data[0]
                api_url = best_api.get("url", "")
                data = best_api.get("data", {})
                
                # Analyze pattern
                from src.autodiscover.analyzer import PatternAnalyzer
                analyzer = PatternAnalyzer()
                pattern = analyzer.detect_list_pattern(data)
                
                if pattern:
                    pattern["title"] = diagnosis.get("title", "")
                    return self.generate_api_adapter(source_url, api_url, pattern)
        
        # Fall back to HTML analysis
        html_file = diag_path / "page.html"
        if html_file.exists():
            with open(html_file, "r", encoding="utf-8") as f:
                html = f.read()
            
            from src.autodiscover.analyzer import PatternAnalyzer
            analyzer = PatternAnalyzer()
            html_patterns = analyzer.analyze_html_for_patterns(html)
            
            if html_patterns.get("tables") or html_patterns.get("cards"):
                pattern = {
                    "title": diagnosis.get("title", ""),
                    "item_selector": html_patterns["cards"][0]["selector"] if html_patterns.get("cards") else "tr",
                    "field_selectors": {},
                }
                return self.generate_html_adapter(source_url, pattern)
        
        logger.warning(f"Could not generate adapter for {source_url}")
        return None


# CLI usage
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        diagnosis_dir = sys.argv[1]
        generator = AdapterGenerator()
        result = generator.generate_from_diagnosis(diagnosis_dir)
        if result:
            print(f"Generated adapter: {result}")
        else:
            print("Failed to generate adapter")
    else:
        print("Usage: python adapter_generator.py <diagnosis_dir>")
